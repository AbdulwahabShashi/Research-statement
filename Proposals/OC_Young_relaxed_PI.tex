\documentclass[./main.tex]{subfiles}

\begin{document}
\begin{center} 
{\Large \textbf{Convergence of policy iteration for pathwise relaxed optimal control in the Young regime} }
\end{center}


\section{Problem formulation and objectives}
Let $\gamma\in (\frac 1  2,1]$ and fix a path $Z\in C^\gamma([0,1];\R^d)$. We consider a controlled Young differential equation where the controls $\mu$ are in some suitable set of measure-valued paths which we denote by $\cU$. We consider the Young controlled differential equation
\[
\dif X_t=\int_U b(X_t,t,a)\mu_t(\diff a)\dif t+\sigma(X_t,t)\dif Z_t, \  \ \ X_0=x, \  \ \ t\in [0,T],
\]
for some suitable function $b,\sigma$ and $U\subset \R^k$. 
It goes without saying that we actually have $X=X^{x,\mu}$, i.e.\ the solution $X$ depends on $\mu$ and the initial condition $x$. 

Define $J:\cU\to \R$ by
\[
J(x,t,\mu):=\int_t^Tf(X_s^{x,\mu},s,\mu_s)\dif s+g(X_T^{x,\mu}),
\]
for some suitable functions $f$ and $g$.

We consider the optimisation problem
\[
v(x,t):=\sup_{\mu\in\cU}J(x,t,\mu).
\]
Under suitable assumptions on $\cU$, from \cite{Mazliak_Nourdin} we get the existence of optimal control $\mu^*\in\cU$, namely
\[
J(x,t,\mu^*)=\sup_{\mu\in\cU}J(x,t,\mu).
\]
The integration in \cite{Mazliak_Nourdin} uses fractional calculus which we are not intending to use as we prefer to work with the ``Riemann-Stieltjes" definition. However, the proof of existence of $\mu^*$ is done in less than ten pages and as such it is harmless to redo them with our notion of integration (even though both of these notions agree).  

We want to relate $v$ to the solution of HJB equation. To that end, we assume that that the set of measure-valued paths $\cU$ is a subset of $C^\beta([0,T];K)$ where $K$ is a compact set of measures. Under those assumptions, furthermore, in \cite{Chakraborty_Honnappa_Tindel} it is shown that the value function $v$ solves the HJB equation, namely in some suitable sense
\[
\partial_t v(x,t)+\sup_{\mu\in K}H(x,t,\mu,\nabla v(x,t))+\nabla v(x,t)\cdot\sigma(x,t)\partial_t Z_t=0, \ \ \ \ v(x,T)=g(x),
\]
where the Hamiltonian $H$ is defined by
\[
H(x,t,\mu,p):= p\cdot\int_U b(x,t,a)\mu(\diff a)+f(x,t,\mu).
\]
The caveat is that in \cite{Chakraborty_Honnappa_Tindel} the proofs are written with a focus on the rough regime, i.e.\ $\gamma\in (\frac 13,\frac 1 2)$. Since our set-up is easier, we might consider revisiting some of their proofs.

\begin{remark}
The term $\partial_t Z_t$ appearing in the HJB equation should be interpreted suitably. The way it is done in \cite{Chakraborty_Honnappa_Tindel} is to consider suitable space of rough test functions that solve a rough PDE. In our situation we may not need to consider such rough test functions, or at least, we may only have to consider them in the Young regime. 
\end{remark}
    


Our main objectives of the paper are the following:
\begin{itemize}
    \item Prove convergence of  the policy iteration for  the optimal control problem formulated above under added viscosity in terms of $\varepsilon\Delta$ to the HJB equation
    \item Prove convergence as $\varepsilon\to 0^+$ under suitable, perhaps stronger, assumptions
    \item If interesting, relate the problem with the additional viscosity $\varepsilon\Delta$ to another control problem, likely stochastic control, where there is an additive $\sqrt{\varepsilon}\dif B_t$ in the equation (this might be tricky as we could possibly have anticipative controls) 
\end{itemize}

 \section{Policy iteration}
The control problem that we are considering is a generalisation of the control problem considered in \cite{Tang_Tran_Zhang} where it is proved that policy iteration scheme converges. We want to apply the same techniques as in \cite{Tang_Tran_Zhang} without their discretisation step. Furthermore, in our case as $Z$ is not necessarily Lipschitz, we fall slightly outside the scope of their work. That makes our work relevant in that regard. 

Before we introduce the algorithm let us define
\[
\alpha(x,t,p):=\operatorname{argmax}_{\mu\in \cU}H(x,t,\mu,p).
\]
\medskip 

\noindent \textbf{PI Algorithm.} Let $\mu^0\in\cU$ be given. We solve for $v^n$ at each step $n=0,1,...$  
\[
\partial_t v^n(x,t)+H(x,t,\mu^n,\nabla v^n(x,t))+\nabla v^n(x,t)\cdot\sigma(x,t)\partial_t Z_t=-\varepsilon \Delta v^n(x,t), \ \ \ \ v^n(x,T)=g(x).
\]
Then we set for the next step
\[
\mu^{n+1}_t(x,\diff a):=\alpha(x,t,\nabla v^n(x,t)).
\]


\section{Future research}
A natural follow-up work is the case $\gamma\in (\frac 13,\frac 1 2)$ where we really need to use rough path analysis as done in \cite{Chakraborty_Honnappa_Tindel}. We expect this case to be more challenging. 

Another question is whether one could understand the convergence of the enhanced PI algorithm that we have from stochastic point of view. More precisely, we mentioned that we are to  some extent adding $\sqrt{\varepsilon}\dif B_t$ to our equation, so the problem might be formulated in terms of a stochastic control problem. However, understanding an equation where one noise, namely $\diff Z_t$, is understood in the pathwise sense and the noise $\diff B_t$ in the Ito sense might require to use the theory of hybrid pathwise-It√¥ calculus. Of course, since the Brownian noise is additive, we may get away without relying on heavy machinary, e.g.\ the recent work \cite{Friz_Hocquet_Le}.  Exploiting such technique might yield an alternative way of proving the same result. 

Finally, in \cite{Friz_Hocquet_Le} it is mentioned that pathwise control problems with two noises arise naturally and as such one could wonder whether our analysis carries over to the case where we do not manually put $\varepsilon\Delta$, but rather obtain the Laplacian from the equation itself, say $a_{ij}\partial_{ij}$ for some $a:=(a_{ij})$ uniformly elliptic. This is a different problem as we do not intend to take $\varepsilon\to 0$ and the stochasticity plays an important role. 




 

\bibliographystyle{alpha}
\bibliography{references}


\end{document}